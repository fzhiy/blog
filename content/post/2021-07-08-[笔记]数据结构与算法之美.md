---
title: "[笔记]数据结构与算法之美"
date: 2021-07-08T20:47:09+08:00
draft: true

toc: true
categories: [Notes]
tags: [Algorithm]
keywords: []
description: ""
---


# [笔记]数据结构与算法之美

> 引用内容归极客时间《数据结构与算法之美》所有，本文仅作记录。

[TOC]



## 大纲

![算法大纲——数据结构与算法之美](https://img.fzhiy.net/img/%E7%AE%97%E6%B3%95%E5%A4%A7%E7%BA%B2%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E.jpg)

## 00. 20个常用数据结构与算法

![image-20210707225928714](https://img.fzhiy.net/img/image-20210707225928714.png)

## 01. 复杂度分析

![01 常见复杂度量级](https://img.fzhiy.net/img/01%20%E5%B8%B8%E8%A7%81%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%87%8F%E7%BA%A7.jpg)

分类：1）多项式量级；2）非多项式量级【只有两个，O(2^n^) 和 O(n!)】

- 最好情况时间复杂度：在最理想的情况下，执行这段代码的时间复杂度
- 最坏情况时间复杂度：在最糟糕的情况下，执行这段代码的时间复杂度
- 平均情况时间复杂度：加权平均 或 期望 时间复杂度
- 均摊时间复杂度 应用场景：对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，**只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系**，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，**平摊**到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，**一般均摊时间复杂度就等于最好情况时间复杂度。**

## 02. 数组

数组（下标从0开始）：线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。

关键词：

1. 线性表
2. 连续的内存空间和相同类型的数据（**随机访问**，插入删除低效）

数组和链表的区别： 链表适合插入、删除，时间复杂度O(1)；数组支持随机访问，<u>根据下标随机访问</u>的时间复杂度为O(1)

插入删除低效的改进方法：

- 插入：如果数组中存储的数据<u>并没有任何规律</u>，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，**直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置**。
- 删除：在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们**将多次删除操作集中在一起执行**，删除的效率效率提高（**JVM标记清除垃圾回收算法的核心思想**）

警惕数组越界

数组与容器

- Java ArrayList将很多动作的细节封装起来，支持动态扩容（涉及内存申请和数据搬移，耗时。如果能事先确定存储大小，在创建ArrayList时指定数据大小）
- 数组适用场合：
  1. Java ArrayList无法存储基本类型，比如int、long，需要封装为Integer、Long类，而Autoboxing、Unboxing则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组
  2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以直接使用数组。
  3. 当要表示多维数组时，用数组往往会更加直观。比如Object[] [] array；而用容器的话则需要这样定义：ArrayList<ArrayList > array
- 对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选

![02 - 线性表](https://img.fzhiy.net/img/02%20-%20%E7%BA%BF%E6%80%A7%E8%A1%A8.jpg)

![03 - 非线性表](https://img.fzhiy.net/img/03%20-%20%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%A1%A8.jpg)

## 03. 链表

链表经典应用场景：LRU缓存淘汰算法

缓存淘汰策略：

- 先进先出策略FIFO
- 最少使用策略LFU
- 最近最少使用策略LRU

链表：并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用

![05 - 数组与链表的内存分布-](https://img.fzhiy.net/img/05%20-%20%E6%95%B0%E7%BB%84%E4%B8%8E%E9%93%BE%E8%A1%A8%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E5%B8%83-.jpg)

### 单链表

![04 - 单链表-](https://img.fzhiy.net/img/04%20-%20%E5%8D%95%E9%93%BE%E8%A1%A8-.jpg)

第一个节点：头节点

最后一个节点：尾节点（指针不是指向下一个结点，而是指向一个空地址NULL，表示这是链表上最后一个结点。）

针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是O(1)。（如下图）

查找操作需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。

![06 - 链表的插入与删除-](https://img.fzhiy.net/img/06%20-%20%E9%93%BE%E8%A1%A8%E7%9A%84%E6%8F%92%E5%85%A5%E4%B8%8E%E5%88%A0%E9%99%A4-.jpg)

### 循环链表

![07 - 循环链表-](https://img.fzhiy.net/img/07%20-%20%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8-.jpg)

一种特殊的单链表（与单链表唯一的区别在尾结点，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点）

和单链表相比，循环链表的优点是**从链尾到链头比较方便**。当**要处理的数据具有环型结构特点**时，就特别适合采用循环链表。比如著名的<u>约瑟夫问题</u>

### 双向链表

双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。

![08 - 双向链表-](https://img.fzhiy.net/img/08%20-%20%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8-.jpg)

如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然<u>两个指针比较浪费存储空间</u>，但可以<u>支持双向遍历</u>，这样也带来了双向链表操作的灵活性

从结构上来看，双向链表可以<u>支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点</u>，也使双向链表<u>**在某些情况下**的插入、删除等操作都要比单链表简单、高效。</u> （对于后者的解读如下）

在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：

- 删除结点中“值等于某个给定值”的结点；
- 删除给定指针指向的结点。

除了插入、删除操作有优势之外，**对于一个<u>有序链表</u>，双向链表的按值查询的效率也要比单链表高一些**。因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。

对于第二种情况，已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点，而**单链表并不支持直接获取前驱结点**，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到p->next=q，说明p是q的前驱结点。**双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历**。所以，针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定

LinkedHashMap的实现使用了双向链表

对于执行较慢的程序，可以通过消耗更多的内存（**空间换时间**，如缓存技术）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗

![09 - 双向循环链表-](https://img.fzhiy.net/img/09%20-%20%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8-.jpg)

### 链表VS数组 性能大比拼

![10 - 性能比拼-](https://img.fzhiy.net/img/10%20-%20%E6%80%A7%E8%83%BD%E6%AF%94%E6%8B%BC-.jpg)

数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。链表本身没有大小的限制，天然地支持扩容。 

如果代码对内存的使用非常苛刻，适合使用数组。

在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。

一种LRU缓存淘汰算法实现思路：维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
2. 如果此数据没有在缓存链表中，又可以分为两种情况：
   - 如果此时缓存未满，则将此结点直接插入到链表的头部；
   - 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部

缓存访问的时间复杂度：因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为O(n)。

继续优化这个实现思路，比如引入**散列表（Hash table）**来记录每个数据的位置，将缓存访问的时间复杂度降到O(1)

### 如何轻松写出正确的链表代码

- 理解指针或引用的含义

  - 将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。

- 警惕指针丢失和内存泄漏

  - 插入结点时，一定要注意操作的顺序
  - 删除链表结点时，也一定要记得手动释放内存空间，否则，也会出现内存泄漏的问题

- 利用哨兵简化实现难度

  带头链表：带有哨兵的链表（哨兵结点是不存储数据的）

  ![11 - 带头链表](https://img.fzhiy.net/img/11%20-%20%E5%B8%A6%E5%A4%B4%E9%93%BE%E8%A1%A8.jpg)

- 重点留意边界条件处理

  > 代码的基本边界条件检查：
  >
  > 如果链表为空时，代码是否能正常工作？
  > 如果链表只包含一个结点时，代码是否能正常工作？
  > 如果链表只包含两个结点时，代码是否能正常工作？
  > 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

- 举例画图，辅助思考

- 多写多练，没有捷径

  > 练习：
  >
  > 单链表反转
  > 链表中环的检测
  > 两个有序的链表合并
  > 删除链表倒数第n个结点
  > 求链表的中间结点

## 04. 栈

![12 - 栈](https://img.fzhiy.net/img/12%20-%20%E6%A0%88.jpg)

栈是一种“操作受限”的线性表，只允许在一端插入和删除数据

当某个数据集合**只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性**，这时我们就应该首选“栈”这种数据结构。

我们说空间复杂度的时候，是指**除了原本的数据存储空间外，算法运行还需要额外的存储空间。**

- 顺序栈

- 链栈

- 支持动态扩容的顺序栈

  ![13 - 支持动态扩容的栈](https://img.fzhiy.net/img/13%20-%20%E6%94%AF%E6%8C%81%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9%E7%9A%84%E6%A0%88.jpg)

  摊还分析：

  如果当前栈大小为K，并且已满，当再有新的数据要入栈时，就需要重新申请2倍大小的内存，并且做K个数据的搬移操作，然后再入栈。但是，接下来的K-1次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这K-1次入栈操作都只需要一个simple-push操作就可以完成。（如下图）

  ![14 - 入栈的时间复杂度](https://img.fzhiy.net/img/14%20-%20%E5%85%A5%E6%A0%88%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6.jpg)

  这K次入栈操作，总共涉及了K个数据的搬移，以及K次simple-push操作。将K个数据搬移均摊到K次入栈操作，那每个入栈操作只需要一个数据搬移和一个simplepush操作。以此类推，入栈操作的均摊时间复杂度就为O(1)

- 栈在软件工程中的应用：函数调用栈，表达式求值，栈在括号匹配中的应用

  - 表达式求值：编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取2个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

    ![15 - 表达式求值](https://img.fzhiy.net/img/15%20-%20%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B1%82%E5%80%BC.jpg)

- 栈的实际应用：实现浏览器的前进和后退

## 05. 队列

- 队列

  ![16 - 队列](https://img.fzhiy.net/img/16%20-%20%E9%98%9F%E5%88%97.jpg)

  顺序队列 

  顺序队列的队尾没有空间的处理 即tail==n的情况（**数据搬移 - 同前面数组一节提到的**，如图）

  ![17 - 数组实现顺序队列](https://img.fzhiy.net/img/17%20-%20%E6%95%B0%E7%BB%84%E5%AE%9E%E7%8E%B0%E9%A1%BA%E5%BA%8F%E9%98%9F%E5%88%97.jpg)

  链式队列

  ![18 - 链式队列实现](https://img.fzhiy.net/img/18%20-%20%E9%93%BE%E5%BC%8F%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0.jpg)

- 循环队列

  刚才用数组来实现队列的时候，在tail==n时，会有数据搬移操作，这样入队操作性能就会受到影响。那有没有办法能够避免数据搬移呢？

  ![19 - 循环队列_1](https://img.fzhiy.net/img/19%20-%20%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97_1.jpg)

  元素a, b 入队后

  ![19 - 循环队列_2](https://img.fzhiy.net/img/19%20-%20%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97_2.jpg)

  这样成功避免了数据搬移操作，但是写出bugfree的代码较难。最关键的是 **确定好队空和队满的判定条件。**

  判定条件：

  - 队空：head == tail
  - 队满：(tail + 1) % n == head   (队满时tail指向的位置是没有存储数据的，浪费了一个数组的存储空间)

- 阻塞队列：在队列基础上增加了阻塞操作。。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。

  ![20 - 阻塞队列](https://img.fzhiy.net/img/20%20-%20%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97.jpg)

  上述定义就是一个“生产者-消费者模型”

  这种基于阻塞队列实现的“生产者-消费者模型”，可以**有效地协调生产和消费的速度**。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。

  基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来**提高数据的处理效率**。

- 并发队列：线程安全的队列

  最简单直接的实现方式是直接在enqueue()、dequeue()方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用CAS原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。

- 队列在线程池等有限资源池中的应用

  当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？   第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。

  那如何存储排队的请求呢？  队列。 顺序队列和链式队列  对于排队请求有什么区别？

  基于链表的实现方式，可以实现一个<u>支持无限排队的无界队列</u>（unbounded queue），但是可<u>能会导致过多的请求排队等待，请求处理的响应时间过长。</u>所以，针对**响应时间比较敏感**的系统，基于链表实现的无限排队的线程池是**不合适的**。

  而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式<u>对响应时间敏感的系统来说，就相对更加合理</u>。不过，设置一个<u>合理的队列大小</u>，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。

  队列可以应用在<u>任何有限资源池</u>中，用于<u>排队请求</u>，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

## 06. 递归

- 递归需要满足的三个条件

  1. 一个问题的解可以分解为几个子问题的解
  2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
  3. 存在递归终止条件

  写递归代码的关键就是**找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。**  在子问题已解决的情况下，不要继续人工递归计算徒增麻烦，而是<u>考虑原问题与子问题之间的关系</u>

- 递归代码要警惕堆栈溢出

- 递归代码要警惕重复计算

- 递归代码虽然简洁高效，但是，递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码的时候，一定要控制好这些副作用。

## 07. 排序

- 排序 第一节

  插入排序和冒泡排序的时间复杂度相同，都是O(n )，<u>在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？</u>

  ![21 - 排序算法？](https://img.fzhiy.net/img/21%20-%20%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%9F.jpg)

  - 如何分析一个“排序算法”？除了原理、代码实现，更重要的是学会如何评价、分析一个排序算法

    分析排序算法从以下几方面入手

    - 排序算法的执行效率

      - 最好情况、最坏情况、平均情况时间复杂度

        第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。

      - 时间复杂度的系数、常数 、低阶

        实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来

      - 比较次数和交换（或移动）次数

        这一节和下一节讲的都是<u>基于比较的排序算法</u>。基于比较的排序算法的执行过程，会涉及两种操作，一种是<u>元素比较大小</u>，另一种是<u>元素交换或移动</u>。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

    - 排序算法的内存消耗

      ==原地排序==算法，就是特指空间复杂度是O(1)的排序算法。本节的 <u>冒泡、插入、选择排序算法都是原地排序算法。</u>

    - 排序算法的稳定性

      ==稳定性==。这个概念是说，<u>如果待排序的序列中存在值相等的元素，经过排序之后，**相等元素之间原有的先后顺序**不变。</u>

      我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？

      最直接想到的方法是 先按照金额对订单排序，然后遍历对每个金额相同的小区间再按照订单时间排序。但是实现起来很复杂

      借助**稳定排序算法**，这个问题可以非常简洁地解决。解决思路是这样的：我们先**按照下单时间给订单排序**，注意是按照下单时间，不是金额。**排序完成之后，我们用稳定排序算法，按照订单金额重新排序**。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金
      额相同的订单按照下单时间从早到晚排序的。为什么呢？**<u>稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变</u>**

      ![22 - 稳定排序算法的用途](https://img.fzhiy.net/img/22%20-%20%E7%A8%B3%E5%AE%9A%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%9A%84%E7%94%A8%E9%80%94.jpg)

  - 冒泡排序：只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让<u>至少一个元素</u>移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。

    - 当某次冒泡操作已经<u>没有数据交换</u>时，说明已经达到完全有序，不用再继续执行后续的冒泡操作

    - **它是原地排序算法，稳定排序算法，最好情况时间复杂度O(n)，最坏情况时间复杂度O(n^2^)，平均时间复杂度O(n^2^)**

      ![23 - 冒泡排序](https://img.fzhiy.net/img/23%20-%20%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F.jpg)

      ==有序度==是数组中具有有序关系的元素对的个数。<u>有序元素对：a[i] <= a[j], 如果i < j。</u>

      逆序度的定义正好跟有序度相反（默认从小到大为有序）。<u>逆序元素对：a[i] > a[j], 如果i < j。</u>

      **逆序度 = 满有序度 - 有序度**

      冒泡排序包含两个操作原子，比较和交换。**每交换一次，有序度就加1。**不管算法怎么改进，**交换次数总是确定的，即为逆序度**，也就是n*(n-1)/2–初始有序度。

      对于包含n个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是0，所以要进行n*(n-1)/2次交换。最好情况下，初始状态的有序度是n*(n-1)/2，就不需要进行交换。我们可以取个中间值n*(n-1)/4，来表示初始有序度既不是很高也不是很低的
      平均情况。 换句话说，平均情况下，需要n*(n-1)/4次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是O(n^2^)，所以平均情况下的时间复杂度就是O(n^2^)。

  - 插入排序：首先将数组中的数据分为两个区间，**已排序区间**和**未排序区间**。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的**核心思想**是**<u>取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序</u>**。重复这个过程，直到未排序区间中元素为空，算法结束。

    ![24 - 动态排序过程](https://img.fzhiy.net/img/24%20-%20%E5%8A%A8%E6%80%81%E6%8E%92%E5%BA%8F%E8%BF%87%E7%A8%8B.jpg)

    要排序的数据是4，5，6，1，3，2。其中左侧为已排序区间，右侧是未排序区间。

    ![25 - 插入排序](https://img.fzhiy.net/img/25%20-%20%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F.jpg)

    对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但<u>对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。</u>

    为什么说移动次数就等于逆序度呢？我拿刚才的例子画了一个图表（如上图）。满有序度是n*(n-1)/2=15，初始序列的有序度是5，所以逆序度是10。插入排序中，数据移动的个数总和也等于10=3+3+4。

    **它是 原地排序算法，稳定排序算法，最好情况时间复杂度O(n)，最坏情况时间复杂度O(n^2^)，平均时间复杂度O(n^2^)**

  - 选择排序

    选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序**每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。**

    ![26 - 选择排序](https://img.fzhiy.net/img/26%20-%20%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F.jpg)

    它是 一种**原地排序**算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为**O(n^2^)。**

    选择排序是一种**<u>不稳定的排序算法</u>**。从前面画的那张图中，你可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。

    比如5，8，5，2，9这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素2，与第一个5交换位置，那第一个5和中间的5顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了

  - 冒泡排序和插入排序的时间复杂度都是O(n^2^)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？

    前面分析冒泡排序和插入排序的时候讲到，冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，**元素移动的次数也等于原始数据的逆序度。**

    但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，**冒泡排序需要3个赋值操作，而插入排序只需要1个**

    ```c
    冒泡排序中数据的交换操作：
    if (a[j] > a[j+1]) { // 交换
        int tmp = a[j];
        a[j] = a[j+1];
        a[j+1] = tmp;
        flag = true;
    }
    插入排序中数据的移动操作：
    if (a[j] > value) {
        a[j+1] = a[j]; // 数据移动
        } else {
        break;
    }
    ```

    我们把执行一个赋值语句的时间粗略地计为单位时间（unit_time），然后分别用冒泡排序和插入排序对同一个逆序度是K的数组进行排序。用冒泡排序，需要K次交换操作，每次需要3个赋值语句，所以交换操作总耗时就是3*K单位时间。而插入排序中数据移动操作只需要K个单位时间。

    进一步地，插入排序地优化见 <u>希尔排序</u>。

  - 对比总结

    ![27 - 3种排序算法的对比总结](https://img.fzhiy.net/img/27%20-%203%E7%A7%8D%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93.jpg)

  - 分治思想（归并排序和快速排序都用到了）用于解决非排序问题，如：<u>如何在O(n)的时间复杂度内查找一个无序数组中的第K大元素？</u>

  - 归并排序

    - 核心思想：如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

      ![28 - 归并排序](https://img.fzhiy.net/img/28%20-%20%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F.jpg)

      分治：分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。

      分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。**分治是一种解决问题的处理思想，递归是一种编程技巧**，这两者并不冲突。

      归并排序的递推公式：merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))，终止条件：p >= r 不用再继续分解

      ```c
      // 归并排序算法, A是数组，n表示数组大小
      merge_sort(A, n) {
      	merge_sort_c(A, 0, n-1)
      }
      // 递归调用函数
      merge_sort_c(A, p, r) {
          // 递归终止条件
          if p >= r then return
          // 取p到r之间的中间位置q
          q = (p+r) / 2
          // 分治递归
          merge_sort_c(A, p, q)
          merge_sort_c(A, q+1, r)
          // 将A[p...q]和A[q+1...r]合并为A[p...r]
          merge(A[p...r], A[p...q], A[q+1...r])
      }
      ```

      merge(A[p...r], A[p...q], A[q+1...r])的操作：我们申请一个临时数组tmp，大小与A[p...r]相同。我们用**两个游标i和j**，分别指向A[p...q]和A[q+1...r]的第一个元素。比较这两个元素A[i]和A[j]，如果A[i]<=A[j]，我们就把A[i]放入到临时数组tmp，并且i后移一位，否则将A[j]放入到数组tmp，j后移一位。继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组tmp中的数据拷贝到原数组A[p...r]中。

      ![29 - 归并排序-merge](https://img.fzhiy.net/img/29%20-%20%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F-merge.jpg)

      merge()合并函数如果借助哨兵，代码将会简单很多。（**<u>tmp数组临时存放A[p...q]和A[q+1...r]，分别在后面加入一个待排数据INT_MAX（最大值），然后每次将tmp两个数组的第一个元素比较，将较小的一个加入到A数组中，重复这个过程直到其中一个数组元素全部加入到A数组中，最后将另一个数组加入A中</u>**），这样就减少了循环的个数。

      **归并排序 是稳定的排序算法。根据前面的图来看，归并排序稳不稳定关键要看merge()函数，也就是两个有序子数组合并成一个有序数组的那部分代码。** 在合并的过程中，如果A[p...q]和A[q+1...r]之间有值相同的元素，那我们可以像伪代码中那样，先把A[p...q]中的元素放入tmp数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。

      > 归并排序时间复杂度的计算公式：
      >
      > T(1) = C； n=1时，只需要常量级的执行时间，所以表示为C。
      > T(n) = 2*T(n/2) + n； n>1
      >
      > 计算过程：
      >
      > T(n) = 2*T(n/2) + n
      > = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n
      > = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n
      > = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n
      > ......
      > = 2^k * T(n/2^k) + k * n
      > ......

      **归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是O(nlogn)。**

      **归并排序不是原地排序算法**。递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。<u>在任意时刻，CPU只会有一个函数在执行，也就只会有一个临时的内存空间在使用</u>。临时内存空间最大也不会超过n个数据的大小，所以**空间复杂度是O(n)**

  - 快速排序

    - 快排的思想是这样的：如果要排序数组中下标从p到r之间的一组数据，我们选择p到r之间的任意一个数据作为pivot（分区点）。我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。

      ![30 - 快速排序](https://img.fzhiy.net/img/30%20-%20%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F.jpg)

      ```c
      // 快速排序，A是数组，n表示数组的大小
      quick_sort(A, n) {
      	quick_sort_c(A, 0, n-1)
      }
      // 快速排序递归函数，p,r为下标
      quick_sort_c(A, p, r) {
      if p >= r then return
          q = partition(A, p, r) // 获取分区点
          quick_sort_c(A, p, q-1)
          quick_sort_c(A, q+1, r)
      }
      
      // 原地分区的实现思路
      partition(A, p, r) {
      	pivot := A[r]
          i := p
          for j := p to r-1 do {
              if A[j] < pivot {
                  swap A[i] with A[j]
                  i := i+1
              }
          }
          swap A[i] with A[r]
          return i
      }
      ```

      原地分区函数 解读：这里的处理有点类似选择排序。我们通过游标i把A[p...r-1]分成两部分。A[p...i-1]的元素都是小于pivot的，我们暂且叫它“已处理区间”，A[i...r-1]是“未处理区间”。我们**每次都从未处理的区间A[i...r-1]中取一个元素A[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是A[i]的位置。**

      在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也讲了一种处理技巧，就是交换，在O(1)的时间复杂度内完成插入操作。这里我们也借助这个思想，**只需要将A[i]与A[j]交换，就可以在O(1)时间复杂度内将A[j]放到下标为i的位置** 

      ![30 - 快速排序-原地分区](https://img.fzhiy.net/img/30%20-%20%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F-%E5%8E%9F%E5%9C%B0%E5%88%86%E5%8C%BA.jpg)

      因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个6的相对先后顺序就会改变。所以，**快速排序并不是一个稳定的排序算法**。

      两个极端情况下的时间复杂度，一个是分区极其均衡，一个是分区极其不均衡。它们分别对应快排的最好情况时间复杂度和最坏情况时间复杂度。那快排的平均情况时间复杂度是多少呢？T(n)在大部分情况下的时间复杂度都可以做到O(nlogn)，只有在极端情况下，才会退化到O(n^2^)。而且，我们也有很多方法将这个概率降到很低，如何来做？我们后面章节再讲。

    - 归并排序 与 快速排序

      ![31 - 归并排序 与 快速排序](https://img.fzhiy.net/img/31%20-%20%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%20%E4%B8%8E%20%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F.jpg)

      可以发现，归并排序的处理过程是**由下到上**的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是**由上到下**的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为O(nlogn)的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以**实现原地排序**，**解决了归并排序占用太多内存的问题。**

    - O(n)时间复杂度内求无序数组中的第K大元素

      分区的思想。比如4， 2， 5， 12， 3这样一组数据，第3大元素就是4。

      选择数组区间A[0...n-1]的最后一个元素A[n-1]作为pivot，对数组A[0...n-1]原地分区，这样数组就分成了三部分，A[0...p-1]、A[p]、A[p+1...n-1]。

      如果p+1=K，那A[p]就是要求解的元素；如果K>p+1, 说明第K大元素出现在A[p+1...n-1]区间，我们再按照上面的思路递归地在A[p+1...n-1]这个区间内查找。同理，如果K<p+1，那我们就在A[0...p-1]区间查找。

      时间复杂度分析：第一次分区查找，我们需要对大小为n的数组执行分区操作，需要遍历n个元素。第二次分区查找，我们只需要对大小为n/2的数组执行分区操作，需要遍历n/2个元素。依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16.……直到区间缩小为1。如果我们把每次分区遍历的元素个数加起来，就是：n+n/2+n/4+n/8+...+1。这是一个等比数列求和，最后的和等于2n-1。所以，上述解决思路的时间复杂度就为O(n)。

- 线性排序

- 排序优化



## 08. 二分查找



## 09. 跳表

红黑树也可以实现快速地插入、删除和查找操作。Redis为什么会选择用跳表来实现有序集合呢？ 为什么不用红黑树呢？



## 10. 散列表



## 11. 哈希算法



## 12. 二叉树



## 13. 红黑树



## 14. 递归树



## 15. 堆和堆排序、应用



## 16. 图



## 17. 字符串匹配



## 18. Trie树



## 19. AC自动机



## 20. 贪心算法



## 21. 分治算法



## 22. 回溯算法



## 23. 动态规划



## 24. 拓扑排序



## 25. 最短路径



## 26. 位图



## 27. 概率统计



## 28. 向量空间



## 29. B+树



## 30. 搜索



## 31. 索引



## 32. 并行算法



## 33. 算法实战

