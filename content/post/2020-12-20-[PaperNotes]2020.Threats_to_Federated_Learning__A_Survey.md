---
title: "[PaperNotes]2020.A Systematic Literature Review on Federated Machine Learning: From A Software Engineering"
date: 2020-12-20T20:47:09+08:00
draft: false

toc: true
categories: [Notes]
tags: [PaperNotes-Series, Survey]
keywords: []
description: ""
---


# Threats to Federated Learning: A Survey

文章链接：https://arxiv.org/abs/2003.02133

作者：[Lingjuan Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+L), [Han Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Qiang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Q)

发表：arXiv

截止当前（2020.12.20）被引次数：24


> Zotero链接: [从我的文库打开](zotero://select/items/1_V2BVSLT9)

| 标签1 | 标签2 | 标签3 | 标签4 |
| ----- | ----- | ----- | ----- |
|       |       |       |       |

 [toc]

## Summary

本文侧重危及FL的威胁的最新进展，关注由FL系统内部人员发起的两种特定威胁:1)试图阻止模型完全被学习的中毒攻击;2)推理攻击。忽略了对推理阶段攻击的讨论，主要集中在训练阶段攻击上 

## Abstract

现存的FL protocol设计存在许多系统内外的攻击者可利用的漏洞来破坏数据隐私。因此FLS设计者对FL算法设计对隐私保护的启示尤为重要。

## Introduction

在通信带宽限制，间歇性网络连接，严格的时延限制情况下，传统中心化机器学习不适用。而FL使得模型训练在产生数据源的设备上进行，这是一种合理的替代ML范式。

### Types of Federated Learning

基于数据特征分布和参与者的数据样本[Yang et al., 2019a]，FL分为

- horizontally federated learning (HFL) 
- vertically federated learning (VFL) 
- federated transfer learning(FTL)

具体可参考：[联邦学习笔记（一） - https://fzhiy.net/archives/18/](https://fzhiy.net/archives/18/)

### Privacy Leakage in FL

最近的工作表明FL可能不能都提供足够的隐私保证，在整个训练过程中进行模型更新的通信仍会暴露敏感信息[Bhowmick et al., 2018; Fredrikson et al., 2015; Melis et al., 2019]甚至导致深度泄露[Zhu et al., 2019], 要么是第三方，要么是中心服务器[McMahan et al., 2018; Agarwal et al., 2018].  [Aono et al., 2018]中显示即使一小部分的原始梯度也会泄露本地数据信息。NeurIPS2019的一箱工作[Zhu et al., 2019]显示恶意攻击者完全可以在较少的迭代中从梯度中偷取得到训练数据。

FL protocol设计可能在两方面有漏洞：

- **(潜在的恶意)服务器**可以随着时间的推移观察单个更新，篡改训练过程，并控制参与者在全局参数上的view;
- 能够观察全局参数并控制其参数上传的任何**参与者**。例如，恶意的参与者可以故意改变他们的输入，或者在全局模型中引入秘密的后门。

**现有的FL调查文献多集中在FL的广义方面，即 如何使得FL工作[Yang et al., 2019a; Li et al., 2019;Kairouz et al., 2019].**

在这篇文章中，我们调查了**危及FL的威胁的最新进展**，以弥补人工智能(AI)研究界在这一主题的理解上的这一重要gap。特别地，本文关注由**FL系统内部人员**发起的两种特定威胁:1)试图阻止模型完全被学习的**中毒攻击**，或者使模型产生对对手有利的推论;2)**推理攻击** 针对参与者隐私。

![](https://img.fzhiy.net/img/Attacks%20against%20server-based%20FL.png)

![](https://img.fzhiy.net/img/20201220100816.png)

## Threat Models

威胁模型概述

### Insider v.s. Outsider

![](https://img.fzhiy.net/img/20201220101029.png)

- 内部攻击
  - server
  - participants（client）
- 外部攻击
  - 窃听这在participant和FL server之间的通信信道上发起的攻击
  - 最终的FL模型作为服务部署时用户发起的攻击

内部攻击通常比外部攻击更强，因为它严格地增强了对手的能力。

由于这种强大的行为，本文对FL攻击的讨论将主要集中在内部攻击上，采取以下三种一般形式之一:

- **Single attack**：一个单一的、不相互勾结的恶意参与者的目标是导致模型错误地对一组高置信度选择的输入进行分类[Bhagoji at el，2018;Bagdasaryan at el，2018];
- **Byzantine attack**：拜占庭恶意参与者的行为可能完全任意，并将其输出与正确的模型更新具有相似的分布，使其难以被检测到[Blanchard et al.， 2017; Chen at el，2017;Chen at el，2018;Yin at el，2018];
- **Sybil attack（女巫攻击）**：对手可以模拟多个虚拟参与者账户，或选择先前受到威胁的参与者对FL发动更强大的攻击[Fung at el，2018;Bagdasaryan at el，2018]

### Semi-honest v.s. Malicious

在半诚实的环境下，对手被认为是被动的或诚实但好奇的（honest-but-curious）。他们试图在不偏离（违背）FL协议的情况下学习其他各方的私有状态。假设被动对手只观察聚合梯度或平均梯度，而不观察其他诚实参与者的训练数据或梯度。

在恶意的环境下，活跃的或恶意的对手试图了解诚实参与者的私有状态，并通过修改、重放或删除消息任意偏离FL协议。这种强攻击模型（adversary model）允许对手进行特别具有破坏性的攻击。

### Training Phase v.s. Inference Phase

训练阶段的攻击试图学习、影响或破坏FL模型本身[Biggio at el，2011]。在训练阶段，攻击者可以通过数据中毒攻击来破坏训练数据集集合的完整性，或者通过模型中毒攻击来破坏学习过程的完整性。攻击者还可以对单个参与者的更新或所有参与者的更新集合发起一系列推理攻击

**推理阶段的攻击**称为**回避/探索性攻击**[Barreno等人，2006]。它们通常不会篡改目标模型，相反，要么导致它产生错误的输出(目标/非目标)，要么收集关于模型特征的证据。这种攻击的有效性在很大程度上**取决于对手可以获得的关于模型的信息**。推理阶段攻击可分为白盒攻击(即具有完全访问权的)

推理阶段的攻击分为

- 白盒攻击，即full access to FL model
- 黑盒攻击和黑盒攻击(即只能够查询的FL模型)。

由服务器维护的模型不仅遭受与一般情况相同的逃避攻击,当目标模型被部署为服务时，FL中的模型广播步骤使任何恶意客户端都可以访问模型。因此，FL需要额外的努力来防御白盒逃避攻击。

本文忽略了对推理阶段攻击的讨论，而主要集中在训练阶段攻击上。

## Poisoning Attacks

根据攻击者的目标分类：

- random attacks ==> 其目标为降低FL模型的准确率
- targeted attacks[Huang et al., 2011]==> 其目标为诱导FL模型输出对手指定的目标标签

一般来说，针对性攻击比随机攻击更难，因为攻击者有特定的目标要达到。训练阶段的中毒攻击可以在数据或模型上。

![](https://img.fzhiy.net/img/20201220102507.png)

如上图，中毒的更新来自：(1)本地数据采集过程中的数据中毒攻击;(2)本地模型训练过程中的模型中毒攻击。 这两种中毒攻击都试图修改目标模型的行为。

### Data Poisoning

数据中毒大致分两类：

- clean-label[Shafahi et al., 2018]
  - 假定对手不能改变任何训练数据的标签（因为通过一个过程，数据被证明属于正确的类别，并且数据样本的中毒必须是不易察觉的）
- dirty-label[Gu et al., 2017]
  - 对手可以在训练集中引入一些它希望用目标标签误分类的数据样本；
  - 常见例子：label-flipping attack [Biggio et al., 2012; Fung et al., 2018]. 即 在保持数据特征不变的情况下，将一类诚实训练实例的标签翻转到另一类诚实训练实例
  - backdoor poisoning [Gu et al., 2017].

通常，在clean inputs上的有毒模型的性能不受影响，这种方式更难检测到攻击。

数据中毒攻击可以由任何FL参与者执行。**对FL模型的影响取决于系统中参与攻击的参与程度，以及被毒害的训练数据的数量**。数据中毒在H2C等参与方较少的环境中影响较小。

### Model Poisoning

模型中毒攻击的目的是在**将本地模型更新发送到服务器**或**在全局模型中插入隐藏后门之前使其中毒**[Bagdasaryan at el.，2018]。

## Inference Attacks

![](https://img.fzhiy.net/img/20201220104544.png)

FL训练期间交换梯度会造成严重的隐私泄露[Phong et al., 2018; Su and Xu, 2018; Melis et al., 2019; Zhu et al., 2019]。如上图。

模型更新可能会泄露关于参与者训练数据的意外特征的额外信息给恶意参与者，深度学习模型似乎在内部可以识别非明显与主要任务相关数据的许多特征。

对手还可以保存FL模型参数的快照，并通过利用连续快照之间的差异进行**属性推断（property inference）**，这等于所有参与者汇总的更新减去对手的更新(图4)

![](https://img.fzhiy.net/img/20201220105110.png)

主要原因是梯度来自参与者的私有数据。在深度学习模型中，给定层的梯度是利用该层的特征和来自上面层的误差来计算的。

在连续的全连接层的案例中，权重的梯度是上一层的误差和特征的内积。类似地，对于卷积层，权重的梯度就是上一层的误差和特征的卷积[Melis et al.,  2019]。

因此，模型更新的观察结果可用于推断大量的私有信息，如类代表、成员以及与训练数据子集相关的属性。更糟糕的是攻击者可以**从共享的梯度推断标签，并恢复原始训练样本**，而不需要任何关于训练集的先验知识[Zhu et al.， 2019]。

### Inferring Class Representatives

Hitaj等人[2017]设计了一种主动推理攻击（active inference attack），称为**生成对抗网络(GAN)攻击**（在深度FL模型上）。在这里，恶意的参与者可以故意损害任何其他参与者。GAN攻击利用了FL学习过程的实时特性，允许敌对方训练GAN，该GAN生成目标训练数据的原型样本，这些样本应该是私有的。生成的样本似乎来自与训练数据相同的分布。因此，**GAN攻击的目标不是重建实际的训练输入，而是只重建类表征（class representatives）**。需要注意的是，GAN攻击假设给定类的整个训练语料库来自单个参与者，只有在所有类成员相似的特殊情况下，GAN构造的代表才与训练数据相似。这类似于一般ML设置中的模型反转攻击[Fredrikson等人，2015]。然而，**这些假设在FL中可能不太实用。GAN攻击需要大量的计算资源，不适用于H2C场景**。

### Inferring Membership

**membership inference attacks**的目标是**在给定一个精确的数据点后，确定该数据点是否用于训练模型[Shokri et al., 2017]**.。例如，攻击者可以推断是否使用特定的患者概要来训练与疾病相关的分类器。

在FL中，攻击者的目标是推断一个特定样本是否属于单方的私有训练数据(如果目标更新是单方的)还是任何一方的私有训练数据(如果目标更新是合计的)。

攻击者在FL系统中可以进行主动和被动隶属关系推理攻击[Nasr et al., 2019; Melis et al., 2019].。在被动的情况下，攻击者只需观察更新后的模型参数，在不改变局部或全局协同训练过程的情况下进行推理。在主动情况下，攻击者可以篡改FL模型训练协议，并对其他参与者实施更强大的攻击。具体来说，攻击者共享恶意更新并强制FL模型共享关于攻击者感兴趣的参与者本地数据的更多信息。这种攻击被称为**梯度上升攻击（gradient ascent attack）**[Nasr et al., 2019],，利用SGD优化更新模型参数的方向与损失梯度相反的事实。

### Inferring Properties

攻击者可以发起被动和主动的属性推断攻击，以推断其他参与者的训练数据的属性，这些属性独立于FL模型类的特征[Melis等人，2019]。属性推断攻击（Property inference attacks）假设对手有正确标记了他想要推断的属性的辅助训练数据。

被动对手只能通过训练二值属性分类器来观察/窃听更新并进行推理。

一个活跃的对手可以使用多任务学习来欺骗FL模型，让它学习更好地分离有和没有属性的数据，从而提取更多的信息。一个敌对的参与者甚至可以在训练过程中推断一个属性何时在数据中出现和消失(例如，识别一个人何时首次出现在用于训练性别分类器的照片中)。属性推理攻击的假设可能会阻碍其在H2C中的应用。

### Inferring Training Inputs and Labels

- Deep Leakage from Gradient(DLG) 提出一种能够在较少迭代过程获取训练输入和标签的优化算法[Zhu et al., 2019].。这种攻击方法远比以前的强。它可以恢复像素级精确的原始图像和标记级匹配原始文本。
- Improved Deep Leakage from Gradient (iDLG)[Zhao et al., 2020]可以利用标签和相应梯度符号之间的关系，从共享梯度中提取标签。iDLG适用于在单热标签上用交叉熵损失训练的任何可微模型，这是分类的一般情况。

**推理攻击通常假定对手具有复杂的技术能力和庞大的计算资源。此外，对手必须被选中进行多轮FL训练**。因此，它不适合H2C场景，但更可能在H2B场景下。此类攻击还强调了在FL训练期间保护共享的梯度的必要性，可能通过同态加密等机制[Yang等人，2019b]。

## Discussions and Promising Directions

- Curse of Dimensionality:
- Threats to VFL
- FL with Heterogeneous Architectures
- Decentralized Federated Learning
- Weakness of Current Defense
- Optimizing Defense Mechanism Deployment

